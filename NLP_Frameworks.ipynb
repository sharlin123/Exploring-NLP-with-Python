{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Frameworks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_-qKHPJcmwk"
      },
      "source": [
        "Part 1 : Exploring Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt4er6vYR9dU"
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U08ZgMYhS_uq"
      },
      "source": [
        "text=\"Machine learning is the study of computer algorithms that improve automatically through experience and by the use of data.It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks. A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers; but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. In its application across business problems, machine learning is also referred to as predictive analytics.\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clZzSHsyTRFq",
        "outputId": "621a6f0d-ecd1-4721-93f6-661ea4765b48"
      },
      "source": [
        "# Tokenization of the corpus \n",
        "tokenized =[]\n",
        "for sentence in text.split('.'):\n",
        "  tokenized.append(simple_preprocess(sentence, deacc = True))\n",
        "  \n",
        "print(tokenized)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['machine', 'learning', 'is', 'the', 'study', 'of', 'computer', 'algorithms', 'that', 'improve', 'automatically', 'through', 'experience', 'and', 'by', 'the', 'use', 'of', 'data'], ['it', 'is', 'seen', 'as', 'part', 'of', 'artificial', 'intelligence'], ['machine', 'learning', 'algorithms', 'build', 'model', 'based', 'on', 'sample', 'data', 'known', 'as', 'training', 'data', 'in', 'order', 'to', 'make', 'predictions', 'or', 'decisions', 'without', 'being', 'explicitly', 'programmed', 'to', 'do', 'so'], ['machine', 'learning', 'algorithms', 'are', 'used', 'in', 'wide', 'variety', 'of', 'applications', 'such', 'as', 'in', 'medicine', 'email', 'filtering', 'and', 'computer', 'vision', 'where', 'it', 'is', 'difficult', 'or', 'unfeasible', 'to', 'develop', 'conventional', 'algorithms', 'to', 'perform', 'the', 'needed', 'tasks'], ['subset', 'of', 'machine', 'learning', 'is', 'closely', 'related', 'to', 'computational', 'statistics', 'which', 'focuses', 'on', 'making', 'predictions', 'using', 'computers', 'but', 'not', 'all', 'machine', 'learning', 'is', 'statistical', 'learning'], ['the', 'study', 'of', 'mathematical', 'optimization', 'delivers', 'methods', 'theory', 'and', 'application', 'domains', 'to', 'the', 'field', 'of', 'machine', 'learning'], ['data', 'mining', 'is', 'related', 'field', 'of', 'study', 'focusing', 'on', 'exploratory', 'data', 'analysis', 'through', 'unsupervised', 'learning'], ['in', 'its', 'application', 'across', 'business', 'problems', 'machine', 'learning', 'is', 'also', 'referred', 'to', 'as', 'predictive', 'analytics'], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AETtxracTRI5",
        "outputId": "5f7fe9a7-e410-4645-cfd3-2d1aac3e9e2f"
      },
      "source": [
        "# Created a dictionary out of the set of tokens\n",
        "from gensim import corpora\n",
        "\n",
        "my_dictionary = corpora.Dictionary(tokenized)\n",
        "print(my_dictionary)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary(96 unique tokens: ['algorithms', 'and', 'automatically', 'by', 'computer']...)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ9lVpVbTRLV",
        "outputId": "78ce2b81-5a65-4391-966d-c3cdce9b0bd9"
      },
      "source": [
        "# Converted dictionary into a Bag of Words corpus\n",
        "BoW_corpus =[my_dictionary.doc2bow(doc, allow_update = True) for doc in tokenized]\n",
        "print(BoW_corpus)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1)], [(8, 1), (11, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)], [(0, 1), (5, 2), (9, 1), (10, 1), (18, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 2), (41, 1), (42, 1)], [(0, 2), (1, 1), (4, 1), (8, 1), (9, 1), (10, 1), (11, 1), (14, 1), (18, 1), (20, 1), (29, 2), (34, 1), (40, 2), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1)], [(8, 2), (9, 3), (10, 2), (11, 1), (33, 1), (36, 1), (40, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1)], [(1, 1), (9, 1), (10, 1), (11, 2), (12, 1), (14, 2), (40, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1)], [(5, 2), (8, 1), (9, 1), (11, 1), (12, 1), (15, 1), (33, 1), (69, 1), (78, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1)], [(8, 1), (9, 1), (10, 1), (18, 1), (29, 1), (40, 1), (75, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1)], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeO-ozwQTRNb",
        "outputId": "41e872b0-8dff-40e4-c5f0-7fda588ec5bf"
      },
      "source": [
        "# Printing every word's frequency\n",
        "from gensim import models\n",
        "import numpy as np\n",
        "  \n",
        "# Word weight in Bag of Words corpus\n",
        "word_weight =[]\n",
        "for doc in BoW_corpus:\n",
        "  for id, freq in doc:\n",
        "    word_weight.append([my_dictionary[id], freq])\n",
        "print(word_weight)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['algorithms', 1], ['and', 1], ['automatically', 1], ['by', 1], ['computer', 1], ['data', 1], ['experience', 1], ['improve', 1], ['is', 1], ['learning', 1], ['machine', 1], ['of', 2], ['study', 1], ['that', 1], ['the', 2], ['through', 1], ['use', 1], ['is', 1], ['of', 1], ['artificial', 1], ['as', 1], ['intelligence', 1], ['it', 1], ['part', 1], ['seen', 1], ['algorithms', 1], ['data', 2], ['learning', 1], ['machine', 1], ['as', 1], ['based', 1], ['being', 1], ['build', 1], ['decisions', 1], ['do', 1], ['explicitly', 1], ['in', 1], ['known', 1], ['make', 1], ['model', 1], ['on', 1], ['or', 1], ['order', 1], ['predictions', 1], ['programmed', 1], ['sample', 1], ['so', 1], ['to', 2], ['training', 1], ['without', 1], ['algorithms', 2], ['and', 1], ['computer', 1], ['is', 1], ['learning', 1], ['machine', 1], ['of', 1], ['the', 1], ['as', 1], ['it', 1], ['in', 2], ['or', 1], ['to', 2], ['applications', 1], ['are', 1], ['conventional', 1], ['develop', 1], ['difficult', 1], ['email', 1], ['filtering', 1], ['medicine', 1], ['needed', 1], ['perform', 1], ['such', 1], ['tasks', 1], ['unfeasible', 1], ['used', 1], ['variety', 1], ['vision', 1], ['where', 1], ['wide', 1], ['is', 2], ['learning', 3], ['machine', 2], ['of', 1], ['on', 1], ['predictions', 1], ['to', 1], ['all', 1], ['but', 1], ['closely', 1], ['computational', 1], ['computers', 1], ['focuses', 1], ['making', 1], ['not', 1], ['related', 1], ['statistical', 1], ['statistics', 1], ['subset', 1], ['using', 1], ['which', 1], ['and', 1], ['learning', 1], ['machine', 1], ['of', 2], ['study', 1], ['the', 2], ['to', 1], ['application', 1], ['delivers', 1], ['domains', 1], ['field', 1], ['mathematical', 1], ['methods', 1], ['optimization', 1], ['theory', 1], ['data', 2], ['is', 1], ['learning', 1], ['of', 1], ['study', 1], ['through', 1], ['on', 1], ['related', 1], ['field', 1], ['analysis', 1], ['exploratory', 1], ['focusing', 1], ['mining', 1], ['unsupervised', 1], ['is', 1], ['learning', 1], ['machine', 1], ['as', 1], ['in', 1], ['to', 1], ['application', 1], ['across', 1], ['also', 1], ['analytics', 1], ['business', 1], ['its', 1], ['predictive', 1], ['problems', 1], ['referred', 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQR6Lr5ETRPl",
        "outputId": "9fe144a0-f7ab-423a-f44b-be5bd56f438c"
      },
      "source": [
        "# Create TF-IDF model\n",
        "tfIdf = models.TfidfModel(BoW_corpus, smartirs ='ntc')\n",
        "  \n",
        "# TF-IDF Word Weight\n",
        "weight_tfidf =[]\n",
        "for doc in tfIdf[BoW_corpus]:\n",
        "  for id, freq in doc:\n",
        "    weight_tfidf.append([my_dictionary[id], np.around(freq, decimals = 3)])\n",
        "print(weight_tfidf) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['algorithms', 0.165], ['and', 0.165], ['automatically', 0.331], ['by', 0.331], ['computer', 0.226], ['data', 0.165], ['experience', 0.331], ['improve', 0.331], ['is', 0.061], ['learning', 0.038], ['machine', 0.061], ['of', 0.122], ['study', 0.165], ['that', 0.331], ['the', 0.331], ['through', 0.226], ['use', 0.331], ['is', 0.085], ['of', 0.085], ['artificial', 0.463], ['as', 0.171], ['intelligence', 0.463], ['it', 0.317], ['part', 0.463], ['seen', 0.463], ['algorithms', 0.117], ['data', 0.235], ['learning', 0.027], ['machine', 0.043], ['as', 0.087], ['based', 0.235], ['being', 0.235], ['build', 0.235], ['decisions', 0.235], ['do', 0.235], ['explicitly', 0.235], ['in', 0.117], ['known', 0.235], ['make', 0.235], ['model', 0.235], ['on', 0.117], ['or', 0.161], ['order', 0.235], ['predictions', 0.161], ['programmed', 0.235], ['sample', 0.235], ['so', 0.235], ['to', 0.126], ['training', 0.235], ['without', 0.235], ['algorithms', 0.211], ['and', 0.106], ['computer', 0.144], ['is', 0.039], ['learning', 0.024], ['machine', 0.039], ['of', 0.039], ['the', 0.106], ['as', 0.078], ['it', 0.144], ['in', 0.211], ['or', 0.144], ['to', 0.113], ['applications', 0.211], ['are', 0.211], ['conventional', 0.211], ['develop', 0.211], ['difficult', 0.211], ['email', 0.211], ['filtering', 0.211], ['medicine', 0.211], ['needed', 0.211], ['perform', 0.211], ['such', 0.211], ['tasks', 0.211], ['unfeasible', 0.211], ['used', 0.211], ['variety', 0.211], ['vision', 0.211], ['where', 0.211], ['wide', 0.211], ['is', 0.096], ['learning', 0.09], ['machine', 0.096], ['of', 0.048], ['on', 0.13], ['predictions', 0.179], ['to', 0.07], ['all', 0.261], ['but', 0.261], ['closely', 0.261], ['computational', 0.261], ['computers', 0.261], ['focuses', 0.261], ['making', 0.261], ['not', 0.261], ['related', 0.179], ['statistical', 0.261], ['statistics', 0.261], ['subset', 0.261], ['using', 0.261], ['which', 0.261], ['and', 0.17], ['learning', 0.039], ['machine', 0.063], ['of', 0.125], ['study', 0.17], ['the', 0.339], ['to', 0.091], ['application', 0.232], ['delivers', 0.339], ['domains', 0.339], ['field', 0.232], ['mathematical', 0.339], ['methods', 0.339], ['optimization', 0.339], ['theory', 0.339], ['data', 0.354], ['is', 0.065], ['learning', 0.04], ['of', 0.065], ['study', 0.177], ['through', 0.242], ['on', 0.177], ['related', 0.242], ['field', 0.242], ['analysis', 0.354], ['exploratory', 0.354], ['focusing', 0.354], ['mining', 0.354], ['unsupervised', 0.354], ['is', 0.061], ['learning', 0.038], ['machine', 0.061], ['as', 0.123], ['in', 0.167], ['to', 0.089], ['application', 0.228], ['across', 0.333], ['also', 0.333], ['analytics', 0.333], ['business', 0.333], ['its', 0.333], ['predictive', 0.333], ['problems', 0.333], ['referred', 0.333]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk6kd0sXTRSs",
        "outputId": "5c0acb72-2a8f-47dd-9e07-bc72140b2381"
      },
      "source": [
        "# Text summarization using gensim\n",
        "from gensim.summarization import summarize, keywords\n",
        "  \n",
        "# Summarize the paragraph\n",
        "print(summarize(text, word_count = 25))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning.\n",
            "Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-KFOBzFc6WQ"
      },
      "source": [
        "Part 2: Exploring spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i24pMFUdc_Rx"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create an nlp object\n",
        "text = nlp(\"Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg5cOVUgdCcS",
        "outputId": "907abe12-9959-442e-a9c7-bba35b7ca5f3"
      },
      "source": [
        "# POS Tagging\n",
        "for token in text:\n",
        "    # Print the token and its part-of-speech tag\n",
        "    print(token.text, \"-->\", token.pos_)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Artificial --> ADJ\n",
            "intelligence --> NOUN\n",
            "refers --> VERB\n",
            "to --> ADP\n",
            "the --> DET\n",
            "simulation --> NOUN\n",
            "of --> ADP\n",
            "human --> ADJ\n",
            "intelligence --> NOUN\n",
            "in --> ADP\n",
            "machines --> NOUN\n",
            "that --> DET\n",
            "are --> AUX\n",
            "programmed --> VERB\n",
            "to --> PART\n",
            "think --> VERB\n",
            "like --> SCONJ\n",
            "humans --> NOUN\n",
            "and --> CCONJ\n",
            "mimic --> VERB\n",
            "their --> DET\n",
            "actions --> NOUN\n",
            ". --> PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bib8LGfjdCn3",
        "outputId": "8fe62cb7-6736-42c9-cb22-38e0d3458d22"
      },
      "source": [
        "# Dependency parsing\n",
        "for token in text:\n",
        "    print(token.text, \"-->\", token.dep_)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Artificial --> amod\n",
            "intelligence --> nsubj\n",
            "refers --> ROOT\n",
            "to --> prep\n",
            "the --> det\n",
            "simulation --> pobj\n",
            "of --> prep\n",
            "human --> amod\n",
            "intelligence --> pobj\n",
            "in --> prep\n",
            "machines --> pobj\n",
            "that --> nsubjpass\n",
            "are --> auxpass\n",
            "programmed --> relcl\n",
            "to --> aux\n",
            "think --> xcomp\n",
            "like --> prep\n",
            "humans --> pobj\n",
            "and --> cc\n",
            "mimic --> conj\n",
            "their --> poss\n",
            "actions --> dobj\n",
            ". --> punct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkFgRJpSdCuC",
        "outputId": "e68ec8ad-a0e5-4f54-a122-0d5de344b92a"
      },
      "source": [
        "# Named Entity Recognition\n",
        "\n",
        "text2 = nlp(\"Indian population originated in three migration waves from Africa, Iran and Asia.\")\n",
        " \n",
        "for ent in text2.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indian NORP\n",
            "three CARDINAL\n",
            "Africa LOC\n",
            "Iran GPE\n",
            "Asia LOC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sDFSF_vdXTO"
      },
      "source": [
        "Part 3: Exploring Wordnet with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmPGaoYNdf01"
      },
      "source": [
        "from nltk.corpus import wordnet as wn"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMrEL7gYeDL0",
        "outputId": "079a9b04-0e5d-4256-c05b-5116722f57e1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ7FJt1xdf5W",
        "outputId": "c32b115e-bd7c-475b-c526-c311563dc079"
      },
      "source": [
        "# Synsets of the word \"good\" in wordnet\n",
        "wn.synsets('good')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('good.n.01'),\n",
              " Synset('good.n.02'),\n",
              " Synset('good.n.03'),\n",
              " Synset('commodity.n.01'),\n",
              " Synset('good.a.01'),\n",
              " Synset('full.s.06'),\n",
              " Synset('good.a.03'),\n",
              " Synset('estimable.s.02'),\n",
              " Synset('beneficial.s.01'),\n",
              " Synset('good.s.06'),\n",
              " Synset('good.s.07'),\n",
              " Synset('adept.s.01'),\n",
              " Synset('good.s.09'),\n",
              " Synset('dear.s.02'),\n",
              " Synset('dependable.s.04'),\n",
              " Synset('good.s.12'),\n",
              " Synset('good.s.13'),\n",
              " Synset('effective.s.04'),\n",
              " Synset('good.s.15'),\n",
              " Synset('good.s.16'),\n",
              " Synset('good.s.17'),\n",
              " Synset('good.s.18'),\n",
              " Synset('good.s.19'),\n",
              " Synset('good.s.20'),\n",
              " Synset('good.s.21'),\n",
              " Synset('well.r.01'),\n",
              " Synset('thoroughly.r.02')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMRzhnWHdf75",
        "outputId": "5ce2e1cf-367d-47b4-fd0a-e001a11226eb"
      },
      "source": [
        "print(wn.synset('good.n.03').definition())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "that which is pleasing or valuable or useful\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNOaySMyeUUI",
        "outputId": "c518865a-dab1-4234-9eee-2c6eaa67f1b7"
      },
      "source": [
        "print(wn.synset('good.n.03').examples()[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weigh the good against the bad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC_7g7FueUXI",
        "outputId": "b541df29-b911-4771-c5f7-948d55f828f8"
      },
      "source": [
        "wn.synset('good.n.03').lemmas()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('good.n.03.good'), Lemma('good.n.03.goodness')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WpBXFSFeUZu",
        "outputId": "ff0d6a76-5bba-40e3-fbaf-22d2a72ec3e7"
      },
      "source": [
        "cat = wn.synset('cat.n.01')\n",
        "print(\"Hypernyms of cat-->\")\n",
        "print(cat.hypernyms())\n",
        "print(\"Hyponyms of cat-->\")\n",
        "print(cat.hyponyms())\n",
        "print(\"Lowest common hypernyms of cat and dog -->\")\n",
        "print(wn.synset('dog.n.01').lowest_common_hypernyms(wn.synset('cat.n.01')))\n",
        "good = wn.synset('good.a.01')\n",
        "print(\"Antonyms of good -->\")\n",
        "print(good.lemmas()[0].antonyms())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hypernyms of cat-->\n",
            "[Synset('feline.n.01')]\n",
            "Hyponyms of cat-->\n",
            "[Synset('domestic_cat.n.01'), Synset('wildcat.n.03')]\n",
            "Lowest common hypernyms of cat and dog -->\n",
            "[Synset('carnivore.n.01')]\n",
            "Antonyms of good -->\n",
            "[Lemma('bad.a.01.bad')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdN4S5q4gmIq",
        "outputId": "042feaa7-0d92-473c-d822-f0267dc701e3"
      },
      "source": [
        "# Similarity between two synsets\n",
        "hit = wn.synset('hit.v.01')\n",
        "slap = wn.synset('slap.v.01')\n",
        "\n",
        "hit.path_similarity(slap)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14285714285714285"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}